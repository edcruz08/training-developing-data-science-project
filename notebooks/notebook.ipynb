{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0IYWMLiaY66_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Developing a Data Science Project: Three Key Phases to Successful Data Products\n",
        "\n",
        "By Dr. Phil Winder of [https://WinderResearch.com](https://WinderResearch.com/?utm_source=notebook&utm_campaign=DevelopingADataScienceProject)\n",
        "\n",
        "Hi there! Welcome to this notebook which was originally written to be used with the training of the same name. If you like this then please [visit my website for more](https://WinderResearch.com/?utm_source=notebook&utm_campaign=DevelopingADataScienceProject). Please feel free to [tweet about this](https://twitter.com/intent/tweet?text=I%27m%20learning%20how%20to%20Develop%20a%20Data%20Science%20Project%20with%20%40DrPhilWinder%20of%20https%3A%2F%2FWinderResearch.com%2F%3Futm_source%3Dtwitter%26utm_campaign%3DDevelopingADataScienceProject), [tweet me direct @DrPhilWinder](https://twitter.com/DrPhilWinder), connect on [Linkedin](https://www.linkedin.com/in/DrPhilWinder/) or plain old [email](mailto:phil@WinderResearch.com). If you need professional help, my company [Winder Research](https://WinderResearch.com/?utm_source=notebook&utm_campaign=DevelopingADataScienceProject) can assist.\n",
        "\n",
        "Table of Contents:\n",
        "\n",
        "1. [Visualising Data](#1.-Visualising-Data)\n",
        "2. [Fixing Scales and Categorical Data](#2:-Fixing-Scales-and-Categorical-Data)\n",
        "3. [Model Improvement through Feature Selection](#3:-Model-Improvement-through-Feature-Selection)\n",
        "4. [Dimensionality Reduction](#4.-Dimensionality-Reduction)\n",
        "5. [Classification](#5.-Classification)\n",
        "6. [Regression](#6.-Regression)\n",
        "7. [Clustering](#7.-Clustering)\n",
        "8. [Numerical Model Evaluation](#8.-Numerical-Model-Evaluation)\n",
        "9. [Visual Model Evaluation](#8.-Visual-Model-Evaluation)"
      ]
    },
    {
      "metadata": {
        "id": "QmiKE8h1Y67C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Need to remove old versions of libraries, they are very out of date on colab.\n",
        "# You will need to restart the kernal after doing this.\n",
        "!pip install seaborn statsmodels pandas sklearn --upgrade\n",
        "data_path = \"https://raw.githubusercontent.com/winderresearch/training-developing-data-science-project/master/data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DpeS8i2TY67I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "sns.set(style=\"white\")\n",
        "matplotlib.rc('figure', figsize=[12, 5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DXeC1tEhY67K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Visualising Data\n",
        "\n",
        "This first section is all about visualising your data. In my opinion, manually visualising data is the most important Data Science technique, but also the most underrepresented.\n",
        "\n",
        "Below I concentrate on demonstrating some techniques that will help you pre-process and clean your data."
      ]
    },
    {
      "metadata": {
        "id": "CCYc49d1Y67L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import missingno as msno\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nfvu3gAbY67O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = data_path + \"titanic.csv\"\n",
        "titanic = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QhCJTT1LY67Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "titanic.hist(color='dimgray', layout=(2, 4));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_FD5C79nY67W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "msno.matrix(titanic.sample(500));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cltcnfdDY67Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "msno.bar(titanic.sample(500));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3KathO-Y67d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "msno.heatmap(titanic);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DTkZ5bXXY67g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.plotting.scatter_matrix(titanic, color='dimgray');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYujbiL7Y67j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix\n",
        "corr = titanic.corr()\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.zeros_like(corr, dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.8, vmin=-0.8, square=True);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wiwFNvsxY67m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = data_path + \"nypd-motor-vehicle-collisions.csv\"\n",
        "collisions = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZ1tPbcSY67p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Challenge\n",
        "\n",
        "Above is the NYPD collisions dataset that reports the causes of traffic incidents in New York (not to be confused with York) in the USA.\n",
        "\n",
        "- Try plotting some visualisations of this data\n",
        "- What can you tell me about it?"
      ]
    },
    {
      "metadata": {
        "id": "iNQAbsAfY67q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t1bkOPkrY67u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "RzNINqI2Y67v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2: Fixing Scales and Categorical Data\n",
        "\n",
        "Fixing the scales of data is important to make it as easy as possible for the model. This is a classification example - where I limit the number of training iterations to make the point - that breaks because the scales are so skewed. After min-max scaling the data, the model finds it much easier to iterate towards the result."
      ]
    },
    {
      "metadata": {
        "id": "9r7ibiwPY67y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn.preprocessing\n",
        "import sklearn.svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5YDi19YY672",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NrQzRcvlY678",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "url = data_path + \"titanic.csv\"\n",
        "titanic = pd.read_csv(url)\n",
        "titanic = titanic[['survived', 'fare', 'age']]\n",
        "titanic.dropna(inplace=True)\n",
        "titanic = titanic[(titanic[['fare', 'age']] != 0).all(axis=1)]  # Remove zero fares\n",
        "y = titanic['survived']\n",
        "X = titanic[['fare', 'age']]\n",
        "\n",
        "clf = sklearn.svm.LinearSVC(tol=1e-2, max_iter=10).fit(X, y)\n",
        "xx, yy = np.mgrid[0:600:1, 0:100:1]\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "probs = clf.predict(grid).reshape(xx.shape)\n",
        "score = clf.score(X, y) * 100\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
        "\n",
        "sns.scatterplot(data=X, x='fare', y='age', hue=y, linewidth=0, ax=ax1).set_title(\n",
        "    \"Titanic Survivors - SVM (10 iter) - {:.1f}%\".format(score))\n",
        "ax1.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
        "\n",
        "X[:] = sklearn.preprocessing.MinMaxScaler().fit_transform(X[:])\n",
        "clf = sklearn.svm.LinearSVC(tol=1e-2, max_iter=10).fit(X, y)\n",
        "xx, yy = np.mgrid[0:1:0.001, 0:1:0.001]\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "probs = clf.predict(grid).reshape(xx.shape)\n",
        "score = clf.score(X, y) * 100\n",
        "\n",
        "sns.scatterplot(data=X, x='fare', y='age', hue=y, linewidth=0, ax=ax2).set_title(\n",
        "    \"Titanic Survivors - SVM (10 iter, scaled) - {:.1f}%\".format(score))\n",
        "ax2.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRLbj252Y68A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s = pd.Series(list('abcaba'))\n",
        "pd.get_dummies(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "twrgu-BKY68J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Above we're rencoding categorical variables into new features. Now we can pass this data into our standard models."
      ]
    },
    {
      "metadata": {
        "id": "yeo54wcFY68L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = data_path + \"titanic.csv\"\n",
        "titanic = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Y0l7zigY68O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = titanic[['age', 'embarked', 'fare', 'survived']]\n",
        "X = X.dropna()\n",
        "y = X[['survived']]\n",
        "X = X[['age', 'embarked', 'fare']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7UxakjqWY68Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Challenge\n",
        "\n",
        "Above we see the Titanic data again. \n",
        "\n",
        "- Try to recode the `embarked` feature (hint: you can run `get_dummies` directly on the X matrix. Pandas is smart enough not to recode the other features)\n",
        "- (Bonus for advanced users) Then use the features X to predict the label y in a classification model"
      ]
    },
    {
      "metadata": {
        "id": "pWHDY07iY68R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zNGQ5ikNY68U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "M0PbqM4_Y68V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3: Model Improvement through Feature Selection\n",
        "\n",
        "The Decision Tree classifier (and variants of) attempts to segment the data into \"pure\" buckets via simple thresholds. The split that produces the most \"pure\" bucket is deemed to be a good split.\n",
        "\n",
        "We can use this definition of \"good\" to provide some information about how well a single feature is able to split segment the data according to the labels. \"Better\" features will have a higher score.\n",
        "\n",
        "The example below uses the titanic dataset again to demonstrate this."
      ]
    },
    {
      "metadata": {
        "id": "m81OxKJ-Y68W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sklearn.tree\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oZ6MY96GY68Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = data_path + \"titanic.csv\"\n",
        "titanic = pd.read_csv(url)\n",
        "X = titanic[['age', 'fare', 'survived']]\n",
        "X = X.dropna()\n",
        "y = X[['survived']]\n",
        "X = X[['age', 'fare']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzPAEhADY68a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mdl = sklearn.tree.DecisionTreeClassifier().fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjE1eaKZY68d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.bar(X.columns, mdl.feature_importances_)\n",
        "plt.gca().set_ylabel('Relative importance');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mu8k7An2Y68g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that the `fare` feature is more informative than the `age` parameter.\n",
        "\n",
        "### Brute Force\n",
        "\n",
        "Another thing we can do is iterate over features to find the best combination. Let's use `mlxtend` to implement this for us (and borrow an example)"
      ]
    },
    {
      "metadata": {
        "id": "7p3w0FIbY68k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install mlxtend > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBID7kDxY68p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from mlxtend.data import wine_data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "X, y = wine_data()\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y, \n",
        "                                                   stratify=y,\n",
        "                                                   test_size=0.3,\n",
        "                                                   random_state=1)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=2)\n",
        "\n",
        "sfs1 = SFS(estimator=knn, \n",
        "           k_features=(3, 10),\n",
        "           forward=True, \n",
        "           floating=False, \n",
        "           scoring='accuracy',\n",
        "           cv=5)\n",
        "\n",
        "pipe = make_pipeline(StandardScaler(), sfs1)\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "print('best combination (ACC: %.3f): %s\\n' % (sfs1.k_score_, sfs1.k_feature_idx_))\n",
        "# print('all subsets:\\n', sfs1.subsets_)\n",
        "plot_sfs(sfs1.get_metric_dict(), kind='std_err');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-t5fyH1Y68r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_feature_names = iris.feature_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oRC5Zpl1Y68s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Challenge\n",
        "\n",
        "Above is the infamous iris dataset.\n",
        "\n",
        "- Can you tell me which feature is the most informative?"
      ]
    },
    {
      "metadata": {
        "id": "ide21ISoY68t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "25fxinelY68w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "pSapLsf-Y68x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Dimensionality Reduction\n",
        "\n",
        "This section introduces the practical applications of dimensionality reduction.\n",
        "\n",
        "The first thing you will notice is that it allows us to take a high-dimensional dataset and visualise it in two dimensions. Visualisation is so important and this is one of the main reasons for performing it.\n",
        "\n",
        "Second, you won't notice much here, but when you start using larger datasets with high numbers of dimensions, you will not want to wait all night just to run one train of your algorithm.\n",
        "\n",
        "We've touched on some concepts before (e.g. collinearity) and they crop up again here."
      ]
    },
    {
      "metadata": {
        "id": "FTNM9DD0Y68x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import decomposition, datasets, linear_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3nlBmWzbY68y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-z0u0ocKY680",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PCA\n",
        "\n",
        "The iris dataset has four components. Let's measure how much each of the four principal components explains the total variance."
      ]
    },
    {
      "metadata": {
        "id": "-PqX4x7qY681",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pca = decomposition.PCA(n_components=4)\n",
        "pca.fit(X)\n",
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uHsSVfdNY684",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first component explains a whopping 92% of the variance. Most rules of thumb would probably get you to just cut it off there. Let's see what the first PC looks like after transforming the data into the new domain (`.transform` effectively does the dot-product projection for us)"
      ]
    },
    {
      "metadata": {
        "id": "g0jC8yIvY684",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_p = pca.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WWW-olPgY688",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(5.5,4))\n",
        "plt.title('Histogram of first PC')\n",
        "plt.hist(X_p[y==0, 0], facecolor='k', label=\"Setosa\")\n",
        "plt.hist(X_p[y==1, 0], facecolor='r', label=\"Versicolour\")\n",
        "plt.hist(X_p[y==2, 0], facecolor='c', label=\"Virginica\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Grjs6Cb0Y68-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that Setosa is well separated. The other two are not quite as separated, but appear to show a good normal distribution.\n",
        "\n",
        "Let's take a look at what the second dimesion looks like."
      ]
    },
    {
      "metadata": {
        "id": "66brBXl9Y69A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.title('Histogram of second PC')\n",
        "plt.hist(X_p[y==0, 1], facecolor='k', label=\"Setosa\")\n",
        "plt.hist(X_p[y==1, 1], facecolor='r', label=\"Versicolour\")\n",
        "plt.hist(X_p[y==2, 1], facecolor='c', label=\"Virginica\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pB30-GIMY69H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how little variance this second dimension shows compared to the first. (I.e. look at the min/max of the first plot and compare to this).\n",
        "\n",
        "Generally you will find that the components with more variance generally have better class separation (because the high variance is accounted by the class separation, not the variance of each class)\n",
        "\n",
        "If you look below, the third looks even worse."
      ]
    },
    {
      "metadata": {
        "id": "M83U2hAmY69H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.title('Histogram of third PC')\n",
        "plt.hist(X_p[y==0, 2], facecolor='k', label=\"Setosa\")\n",
        "plt.hist(X_p[y==1, 2], facecolor='r', label=\"Versicolour\")\n",
        "plt.hist(X_p[y==2, 2], facecolor='c', label=\"Virginica\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyTjcmhBY69L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's take a look at the data in two dimensions."
      ]
    },
    {
      "metadata": {
        "id": "uLErEZf9Y69M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(5.5,4))\n",
        "plt.title('Scatter plot of the first two dimensions of the iris dataset')\n",
        "plt.scatter(X_p[y==0, 0], X_p[y==0, 1], c='k', label=\"Setosa\")\n",
        "plt.scatter(X_p[y==1, 0], X_p[y==1, 1], c='r', label=\"Versicolour\")\n",
        "plt.scatter(X_p[y==2, 0], X_p[y==2, 1], c='c', label=\"Virginica\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lpdLbXn4Y69S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, you can see that there is a large variance in the first (x) dimension, a range of +/- 4, whereas the second dimension (y) only has +/- 1."
      ]
    },
    {
      "metadata": {
        "id": "l1BnmNGlY69T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### PCA with Classification\n",
        "\n",
        "Now we will:\n",
        "\n",
        "- Using a logistic classifier, classify the iris dataset\n",
        "- Calculate the accuracy score (or score of your choice)\n",
        "- Now perform PCA and reduce to a single component. Repeat the classification and scoring.\n",
        "- How much different is the result? Is it significant?"
      ]
    },
    {
      "metadata": {
        "id": "uxtJHygWY69U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mdl = linear_model.LogisticRegression()\n",
        "print(\"No PCA accuracy:\", mdl.fit(X, y).score(X, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcY1-MgOY69X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mdl = linear_model.LogisticRegression()\n",
        "print(\"With PCA (first componennt) accuracy:\", mdl.fit(X_p[:,0].reshape(150,1), y).score(X_p[:,0].reshape(150,1), y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vEoRIlbiY69a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mdl = linear_model.LogisticRegression()\n",
        "print(\"With PCA (two components) accuracy:\", mdl.fit(X_p[:,0:1], y).score(X_p[:,0:1], y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hzR3QdwEY69e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see that we have marginally reduced accuracy. Often, people like to reduce the number of dimensions to 2 or 3 for plotting, then increase it back up for actual classification.\n",
        "\n",
        "This becomes a compromise between simplicity/performance and accuracy."
      ]
    },
    {
      "metadata": {
        "id": "qTzzOGa5Y69e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-gIe2WGrY69i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Challenge\n",
        "\n",
        "Above is the slightly more complex wine dataset. \n",
        "\n",
        "- How much of the variance is represented by the first principal component?\n",
        "- (Bonus) Plot the first principal component"
      ]
    },
    {
      "metadata": {
        "id": "EQCGYRskY69k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LbFeZeqhY69m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1W_EwjYrY69p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hx3OpjGY69u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "TLnJlNN-Y69u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5. Classification\n",
        "\n",
        "This section introduces classification, the task of attempting to predict which class an observation belongs to. Usually there are significantly less classes than observations, because the models need lots of examples to learn from.\n",
        "\n",
        "There are tens of core types of classification model, possibly hundreds of known variations. They are all trying to place a decision boundary within the dimensions of your data to separate the classes.\n",
        "\n",
        "Generally speaking, you want to pick the simplest model you can to solve your problem. In these examples we will use a Decision Tree. They are very simple to undertand and perform remarkably well in most situations.\n",
        "\n",
        "We'll have a look at a complex dataset first, then you can look at the oranges and apples dataset."
      ]
    },
    {
      "metadata": {
        "id": "cg6kz3znY69w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn.tree\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85pdANFCY69y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = data_path + \"cereal.csv\"\n",
        "cereal = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pmTBdIv6Y691",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(cereal.columns)\n",
        "cereal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VR-jsEvKY694",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = cereal[['potass','rating']]\n",
        "mfrs = cereal.mfr.astype('category')\n",
        "y = mfrs.cat.codes\n",
        "print(\"There are {} manufacturers in this dataset.\".format(len(set(y))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4dMgeG6cY697",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a decision tree classifier and train\n",
        "clf = sklearn.tree.DecisionTreeClassifier().fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dp0W1njPY69-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a mesh grid to see what the prediction would be at all of these points\n",
        "xx, yy = np.mgrid[-50:400:1, 0:100:1]\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# Get the predictions at each point\n",
        "probs = clf.predict(grid).reshape(xx.shape)\n",
        "\n",
        "# Calculate the accuracy for plotting in the title\n",
        "score = clf.score(X, y) * 100\n",
        "\n",
        "# Plot the decision boundary and the datapoints\n",
        "ax = plt.gca()\n",
        "ax.contourf(xx, yy, probs, cmap=plt.cm.Paired, alpha=0.2)\n",
        "sns.scatterplot(data=X, x='potass', y='rating', hue=mfrs, linewidth=0).set_title(\n",
        "    \"Cereal Classification - Decision Tree - {:.1f}%\".format(score))\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lTwM5nKAY6-F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So both the classifiers above have scored 100%. But all is not as it seems. First, remember that we should be sceptical about 100%. Next, look at how complicated that decision boundary is for the second example.\n",
        "\n",
        "The solution to this is to split the dataset. We split into a training dataset and a testing dataset."
      ]
    },
    {
      "metadata": {
        "id": "Kk7vUNDmY6-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C1QH2uhcY6-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = sklearn.tree.DecisionTreeClassifier().fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t6pXvSkcY6-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = clf.score(X_test, y_test) * 100\n",
        "print(\"{:.1f}%\".format(score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-2EiG9ZZY6-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Look at that score now! Far lower. There are seven manufacturers here, so randomly picking one would result in a score of 1/7=14%. So we're doing better than random, but certainly not 100%."
      ]
    },
    {
      "metadata": {
        "id": "t_G2HcqHY6-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import some fruit data\n",
        "url = data_path + \"fruits.tsv\"\n",
        "fruit = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# Throw away all data except apples and oranges\n",
        "fruit = fruit.query('(fruit_name == \"apple\") | (fruit_name == \"orange\")')\n",
        "fruit_names = fruit.fruit_name\n",
        "\n",
        "# Throw away all features except mass and color_score\n",
        "X = fruit[['mass', 'color_score']]\n",
        "y = fruit.fruit_label\n",
        "# Recode the orange label (a `3`) to a 1 so we have 0=apple, 1=orange\n",
        "y = y.replace(3, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eQIogFCgY6-Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Challenge\n",
        "\n",
        "Can you now try and classifiy the orange and apples dataset yourself?\n",
        "\n",
        "- Only get the \"score\" to keep this quick and simple. Don't worry about plotting right now.\n",
        "- Try with and without a train test split if you have time"
      ]
    },
    {
      "metadata": {
        "id": "sH74Ta_8Y6-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KuMY0oxOY6-T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "pe6PJbEHY6-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6. Regression\n",
        "\n",
        "Often I provide examples of standard regression with some sort of model. I.e. drawing the best fit line through a series of points. \n",
        "\n",
        "But this time I'd like to show you about exponential smoothing. This is a useful tool for generating good short-term predictions in a simple way.\n",
        "\n",
        "The simplest form of exponential smoothing would predict an average of the past few values, so only the first prediction would be reasonable. The `Holt` method adds a trend element to the model to attempt to model the trend. This is plotted below. The most advanced \"simple\" method attempts to model the seasonality in the data too, but for this you need to tell the model over what period the seasonality fluctuates."
      ]
    },
    {
      "metadata": {
        "id": "BQ0DjO3dY6-V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "\n",
        "url = data_path + \"airline-passengers.csv\"\n",
        "airline = pd.read_csv(url, parse_dates=['Month'], index_col='Month')\n",
        "airline.index.freq = 'MS'\n",
        "train, test = airline.iloc[:130, 0], airline.iloc[130:, 0]\n",
        "mdl_holt = Holt(train).fit()\n",
        "mdl_holtwinters = ExponentialSmoothing(train, seasonal='mul', seasonal_periods=12).fit()\n",
        "pred_holt = mdl_holt.predict(start=test.index[0], end=test.index[-1])\n",
        "pred_holtwinters = mdl_holtwinters.predict(start=test.index[0], end=test.index[-1])\n",
        "\n",
        "plt.plot(train.index, train, label='Train')\n",
        "plt.plot(test.index, test, label='Test')\n",
        "plt.plot(pred_holt.index, pred_holt, label='Holt')\n",
        "plt.plot(pred_holtwinters.index, pred_holtwinters, label='Holt-Winters')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULNeJR2rY6-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = data_path + \"sunspots.csv\"\n",
        "sunspots = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
        "sunspots = sunspots[['Monthly Mean Total Sunspot Number']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FynrCuKzY6-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Challenge\n",
        "\n",
        "- Can you make any predictions on this dataset?\n",
        "\n",
        "Hints: This is a bit tricky because the data isn't quite as nice. It is seasonal, but the seasonality isn't quite as clear as the airline data.\n",
        "\n",
        "- First plot the data to get a closer look. (`sunspots.plot()` and `sunspots.iloc[0:300].plot()`)\n",
        "- Next, use the simple exponential smoothing (`Holt`) and plot the result. It should show a straight line from the last point.\n",
        "- Finally, try and create a full on Holt-Winters (`ExponentialSmoothing`). Use no trend parameter, there is little to no trend. Use an `additive` seasonal parameter. Try setting the `seasonal_periods` to 12 (one year). What happens? What about 150 (or there abouts, approx 12 years)?"
      ]
    },
    {
      "metadata": {
        "id": "r68_3X9HY6-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGfDHT3ZY6-a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gaBU4_1UY6-e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "8oS3XtVXY6-e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 7. Clustering\n",
        "\n",
        "Dendrograms are hierarchical plots of clusters where the length of the bars represent the distance to the next cluster centre.\n",
        "\n",
        "We can lean on our other general purpose data science library `scipy` to provide us with a method to plots dendrograms. Unfortunately we also have to use `scipy`s linkage methods, rather than `sklearn`s because of some expected parameters."
      ]
    },
    {
      "metadata": {
        "id": "3Y_WqAk8Y6-f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "linkage_matrix = linkage(X, 'ward')\n",
        "figure = plt.figure(figsize=(7.5, 5))\n",
        "dendrogram(\n",
        "    linkage_matrix,\n",
        "    color_threshold=0,\n",
        ")\n",
        "plt.title('Hierarchical Clustering Dendrogram (Ward)')\n",
        "plt.xlabel('sample index')\n",
        "plt.ylabel('distance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQxPVb--Y6-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wooohey that's a lot of legs. Let's cut a few off to be able to take a better look at the data..."
      ]
    },
    {
      "metadata": {
        "id": "C4AQt0QWY6-m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(7.5, 5))\n",
        "dendrogram(\n",
        "    linkage_matrix,\n",
        "    truncate_mode='lastp',  # show only the last p merged clusters\n",
        "    p=24,  # show only the last p merged clusters\n",
        "    leaf_rotation=90.,\n",
        "    leaf_font_size=12.,\n",
        "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
        ")\n",
        "plt.title('Hierarchical Clustering Dendrogram (Ward, aggrogated)')\n",
        "plt.xlabel('sample index or (cluster size)')\n",
        "plt.ylabel('distance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gvu5fM7BY6-o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, now we've honed our artistic skills, let's put them to the test in the whiskey data set.\n",
        "\n",
        "## A look at the whiskey dataset\n",
        "\n",
        "Let's load it in again."
      ]
    },
    {
      "metadata": {
        "id": "wSVerp46Y6-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = data_path + \"whiskies.csv\"\n",
        "whiskey = pd.read_csv(url)\n",
        "cols = ['Body', 'Sweetness', 'Smoky', 'Medicinal', 'Tobacco',\n",
        "       'Honey', 'Spicy', 'Winey', 'Nutty', 'Malty', 'Fruity', 'Floral']\n",
        "X = whiskey[cols]\n",
        "distillery_labels = whiskey['Distillery'].as_matrix()\n",
        "display(X.head())\n",
        "display(distillery_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bMPEwd34Y6-s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Challenge:\n",
        "\n",
        "- Plot a dendrogram of the whiskey data\n",
        "\n",
        "Hint: pass the parameter `labels=distillery_labels` to write the distillery names rather than index numbers. And remove the `truncate_mode` and `p` parameters."
      ]
    },
    {
      "metadata": {
        "id": "e3JT2EVTY6-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B45pgIcNY6-w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 8. Numerical Model Evaluation\n",
        "\n",
        "Being able to quantify performance is critical in the process of developing a model. You need to know how well you are solving the problem. You can only do that if you have some ground truth to compare yourself against (i.e. labels).\n",
        "\n",
        "If at all possible, always try and convert the evaluation metric into a unit that everyone can understand. Most often this is profit/loss, but that depends on what domain you are working in. It could be \"number of new subscribers\" or \"lives saved\".\n",
        "\n",
        "In this section we're going to take a quick look at generating a confusion matrix and some technical measures. Confusion matrices help you how your model made an error."
      ]
    },
    {
      "metadata": {
        "id": "jk-VjJZRY6-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model, tree\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVfRxI6lY6-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "mdl = linear_model.LogisticRegression()\n",
        "print(\"Accuracy:\", mdl.fit(X_train, y_train).score(X_test, y_test))\n",
        "y_pred = mdl.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHyjp6jXY6-1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GpXlPcN4Y6--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The confusion matrix above deserves some explanation. This is the digits dataset (8x8 images of written digits) and we're trying to predict which digit it is. So the label in this case is the number from zero to nine. I.e. ten classes.\n",
        "\n",
        "The [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) generated by sklearn is different from the way you would see it in the literature. This is zero indexed and the actual class is down the rows. So the top left cell should be a value of 0 and once it predicted that it was a 4. The bottom left row should be a 9 and it was predicted as a 9 63 times. But 5 times it was predicted to be an 8.\n",
        "\n",
        "Now let's generate some technical metrics."
      ]
    },
    {
      "metadata": {
        "id": "gfW2IJeCY6--",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: {:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))\n",
        "print(\"Balanced Accuracy: {:0.3f}\".format(metrics.balanced_accuracy_score(y_test, y_pred)))\n",
        "print(\"F1 Score: {:0.3f}\".format(metrics.f1_score(y_test, y_pred, average=\"macro\")))\n",
        "print(\"Precision: {:0.3f}\".format(metrics.precision_score(y_test, y_pred, average=\"macro\")))\n",
        "print(\"Recall: {:0.3f}\".format(metrics.recall_score(y_test, y_pred, average=\"macro\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwf8iymsY6_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Only when performance is generally very good do we appoach similar scores across these variety of metrics.\n",
        "\n",
        "Note that we could have calculated more (e.g. AUC), but it would require more code, because some sklearn metrics only work with binary classes."
      ]
    },
    {
      "metadata": {
        "id": "urT4VIz5Y6_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import some fruit data\n",
        "url = data_path + \"fruits.tsv\"\n",
        "fruit = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# Throw away all data except apples and oranges\n",
        "fruit = fruit.query('(fruit_name == \"apple\") | (fruit_name == \"orange\")')\n",
        "fruit_names = fruit.fruit_name\n",
        "\n",
        "# Throw away all features except mass and color_score\n",
        "X = fruit[['mass', 'color_score']]\n",
        "y = fruit.fruit_label\n",
        "# Recode the orange label (a `3`) to a 1 so we have 0=apple, 1=orange\n",
        "y = y.replace(3, 0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "GTglKgx4Y6_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = tree.DecisionTreeClassifier().fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fMLGX7OIY6_E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Challenge:\n",
        "\n",
        "- Generate the confusion matrix for the data above. How is the model getting it wrong?\n",
        "- Generate some metrics for the results above. What does that tell you?"
      ]
    },
    {
      "metadata": {
        "id": "6CZz1sWnY6_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H8PsLREaY6_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "EQfGwxEAY6_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9. Visual Evaluation\n",
        "\n",
        "Visual evaluation can often be more useful because your eyes find it easier to spot differences or issues better than when just looking at raw numbers. Here we're going to look at plotting a ROC curve, which is the number one way of visualising classifier performance.\n",
        "\n",
        "Other visualisations do exist and can be better in certain situations. For example, you wouldn't want to show this plot to a non-technical audience because of the unintuitive use of FPR and TPR metrics.\n",
        "\n",
        "Furthermore, you would use different plots to evaluation the performance of regression and clustering, all of which we don't have time to go into.\n",
        "\n",
        "Finally, check out [scikit plot](https://github.com/reiinakano/scikit-plot) for some useful oneliners to do the same thing as we're doing below."
      ]
    },
    {
      "metadata": {
        "id": "2pz2yt2zY6_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn import decomposition, datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGIgK_MAY6_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.load_digits(return_X_y=True)\n",
        "y[y<5] = 0\n",
        "y[y>=5] = 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "kkFu87XMY6_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mdl = linear_model.LogisticRegression()\n",
        "print(\"Accuracy:\", mdl.fit(X_train, y_train).score(X_test, y_test))\n",
        "y_proba = mdl.predict_proba(X_test)[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nn5L9O3GY6_J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_proba)\n",
        "plt.figure();\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', label='Random')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYD9CbSgY6_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Challenge:\n",
        "\n",
        "- Train another model and plot the ROC curve. (bonus, plot the ROC curves for both models on the same plot)\n",
        "- Which is better?"
      ]
    },
    {
      "metadata": {
        "id": "5-pQbArVY6_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dlb4HNZyY6_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aqk7NhAmY6_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---"
      ]
    },
    {
      "metadata": {
        "id": "M6TlevneY6_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusions and Contact\n",
        "\n",
        "That's it for today. That really was a whistlestop tour of doing Data Science with Python.\n",
        "\n",
        "As you can imagine, we have only touched the surface here. I could spend full days talking about each of these chapters.\n",
        "\n",
        "I hope you've enjoyed it. I hope this makes you want to learn more!\n",
        "\n",
        "If you need professional help, my company [Winder Research](https://WinderResearch.com) can assist. Otherwise, feel free to get in touch using the contact details at the top.\n",
        "\n",
        "Thanks,\n",
        "Phil"
      ]
    }
  ]
}